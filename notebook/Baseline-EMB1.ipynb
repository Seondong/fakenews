{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://www.kaggle.com/c/fake-news-pair-classification-challenge/data\n",
    "* https://github.com/fxsjy/jieba\n",
    "\n",
    "Embedding options\n",
    "* https://github.com/Embedding/Chinese-Word-Vectors (Embedding 1)\n",
    "* https://github.com/facebookresearch/fastText/blob/master/docs/crawl-vectors.md (Embedding 2)\n",
    "\n",
    "* Performance\n",
    "    * Using Embedding 1: Performance 0.7\n",
    "    * Using Embedding 2: Performance 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../data/train.csv'\n",
    "test_path = '../data/test.csv'\n",
    "sample_submission_path = '../data/sample_submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tid1</th>\n",
       "      <th>tid2</th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>title1_en</th>\n",
       "      <th>title2_en</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017养老保险又新增两项，农村老人人人可申领，你领到了吗</td>\n",
       "      <td>警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京</td>\n",
       "      <td>There are two new old-age insurance benefits f...</td>\n",
       "      <td>Police disprove \"bird's nest congress each per...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>Shenzhen's GDP outstrips Hong Kong? Shenzhen S...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>GDP首超香港？深圳澄清：还差一点点……</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>The GDP overtopped Hong Kong? Shenzhen clarifi...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>去年深圳GDP首超香港？深圳统计局辟谣：还差611亿</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>Shenzhen's GDP topped Hong Kong last year? She...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>\"用大蒜鉴别地沟油的方法,怎么鉴别地沟油</td>\n",
       "      <td>吃了30年食用油才知道，一片大蒜轻松鉴别地沟油</td>\n",
       "      <td>\"How to discriminate oil from gutter oil by me...</td>\n",
       "      <td>It took 30 years of cooking oil to know that o...</td>\n",
       "      <td>agreed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>深圳GDP首超香港？统计局辟谣：未超但差距再度缩小</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>Shenzhen's GDP overtakes Hong Kong? Bureau of ...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>\"吃榴莲的禁忌,吃错会致命!</td>\n",
       "      <td>榴莲不能和什么一起吃 与咖啡同吃诱发心脏病\"\"</td>\n",
       "      <td>\"if you eat durian, you will kill yourself if ...</td>\n",
       "      <td>Durian can't eat with anything, it's the same ...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>深圳GDP首超香港？辟谣：未超但差距再度缩小</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>Shenzhen's GDP outpaces Hong Kong? Defending R...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>\"旅行青蛙？居然是一款\"\"生育意愿测试器”！大家还是玩\"\"珠宝V课\"\"吧\"</td>\n",
       "      <td>咸宁一家店的蛋糕含有“棉花”？崇阳多部门联合辟谣</td>\n",
       "      <td>\"Frog frog? It's a fertility test! Let's play\"...</td>\n",
       "      <td>A store in xianning contains \"cotton\"? A multi...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>\"用大蒜鉴别地沟油的方法,怎么鉴别地沟油</td>\n",
       "      <td>一颗大蒜就能鉴别地沟油？别闹了！做到下面几点，让您远离地沟油</td>\n",
       "      <td>\"How to discriminate oil from gutter oil by me...</td>\n",
       "      <td>A single piece of garlic can spot gutter oil? ...</td>\n",
       "      <td>agreed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  tid1  tid2                              title1_zh  \\\n",
       "0   0     0     1          2017养老保险又新增两项，农村老人人人可申领，你领到了吗   \n",
       "1   3     2     3      \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   \n",
       "2   1     2     4      \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   \n",
       "3   2     2     5      \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   \n",
       "4   9     6     7                   \"用大蒜鉴别地沟油的方法,怎么鉴别地沟油   \n",
       "5   4     2     8      \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   \n",
       "6   6     9    10                         \"吃榴莲的禁忌,吃错会致命!   \n",
       "7   5     2    11      \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   \n",
       "8   7    12    13  \"旅行青蛙？居然是一款\"\"生育意愿测试器”！大家还是玩\"\"珠宝V课\"\"吧\"   \n",
       "9   8     6    14                   \"用大蒜鉴别地沟油的方法,怎么鉴别地沟油   \n",
       "\n",
       "                        title2_zh  \\\n",
       "0        警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京   \n",
       "1       深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小   \n",
       "2            GDP首超香港？深圳澄清：还差一点点……   \n",
       "3      去年深圳GDP首超香港？深圳统计局辟谣：还差611亿   \n",
       "4         吃了30年食用油才知道，一片大蒜轻松鉴别地沟油   \n",
       "5       深圳GDP首超香港？统计局辟谣：未超但差距再度缩小   \n",
       "6         榴莲不能和什么一起吃 与咖啡同吃诱发心脏病\"\"   \n",
       "7          深圳GDP首超香港？辟谣：未超但差距再度缩小   \n",
       "8        咸宁一家店的蛋糕含有“棉花”？崇阳多部门联合辟谣   \n",
       "9  一颗大蒜就能鉴别地沟油？别闹了！做到下面几点，让您远离地沟油   \n",
       "\n",
       "                                           title1_en  \\\n",
       "0  There are two new old-age insurance benefits f...   \n",
       "1  \"If you do not come to Shenzhen, sooner or lat...   \n",
       "2  \"If you do not come to Shenzhen, sooner or lat...   \n",
       "3  \"If you do not come to Shenzhen, sooner or lat...   \n",
       "4  \"How to discriminate oil from gutter oil by me...   \n",
       "5  \"If you do not come to Shenzhen, sooner or lat...   \n",
       "6  \"if you eat durian, you will kill yourself if ...   \n",
       "7  \"If you do not come to Shenzhen, sooner or lat...   \n",
       "8  \"Frog frog? It's a fertility test! Let's play\"...   \n",
       "9  \"How to discriminate oil from gutter oil by me...   \n",
       "\n",
       "                                           title2_en      label  \n",
       "0  Police disprove \"bird's nest congress each per...  unrelated  \n",
       "1  Shenzhen's GDP outstrips Hong Kong? Shenzhen S...  unrelated  \n",
       "2  The GDP overtopped Hong Kong? Shenzhen clarifi...  unrelated  \n",
       "3  Shenzhen's GDP topped Hong Kong last year? She...  unrelated  \n",
       "4  It took 30 years of cooking oil to know that o...     agreed  \n",
       "5  Shenzhen's GDP overtakes Hong Kong? Bureau of ...  unrelated  \n",
       "6  Durian can't eat with anything, it's the same ...  unrelated  \n",
       "7  Shenzhen's GDP outpaces Hong Kong? Defending R...  unrelated  \n",
       "8  A store in xianning contains \"cotton\"? A multi...  unrelated  \n",
       "9  A single piece of garlic can spot gutter oil? ...     agreed  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)\n",
    "df_samplesub = pd.read_csv(sample_submission_path)\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>347448</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>347449</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>359100</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>359101</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>359102</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>359103</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>359104</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>359105</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>359106</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>359107</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   Category\n",
       "0  347448  unrelated\n",
       "1  347449  unrelated\n",
       "2  359100  unrelated\n",
       "3  359101  unrelated\n",
       "4  359102  unrelated\n",
       "5  359103  unrelated\n",
       "6  359104  unrelated\n",
       "7  359105  unrelated\n",
       "8  359106  unrelated\n",
       "9  359107  unrelated"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_samplesub.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unrelated    219313\n",
       "agreed        92973\n",
       "disagreed      8266\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding=utf-8\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.993 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2017', '养老保险', '又', '新增', '两项', '，', '农村', '老人', '人人', '可', '申领', '，', '你', '领到', '了', '吗']\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut(\"2017养老保险又新增两项，农村老人人人可申领，你领到了吗\", cut_all=False)\n",
    "print(list(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((320552, 8), (80126, 7))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictt = {}\n",
    "\n",
    "for df in [df_train, df_test]:\n",
    "    for i in df[['tid1','title1_zh']].drop_duplicates().iterrows():\n",
    "        dictt[i[1]['tid1']] = i[1]['title1_zh']\n",
    "    for i in df[['tid2','title2_zh']].drop_duplicates().iterrows():\n",
    "        dictt[i[1]['tid2']] = i[1]['title2_zh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189305"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictt2 = {}\n",
    "for i in dictt.keys():\n",
    "    try:\n",
    "        seg_list = jieba.cut(dictt[i], cut_all=False)\n",
    "        dictt2[i] = list(seg_list)\n",
    "    except AttributeError:\n",
    "        dictt2[i] = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "tmp_file = '../data/merge_sgns_bigram_char300.txt'\n",
    "model = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "model.get_vector('的')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15.0, 26.0, 14.929119674599193, 95)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aafa = []\n",
    "for l in dictt2.values():\n",
    "    aafa.append(len(l))\n",
    "    \n",
    "np.median(aafa), np.percentile(aafa, q=99.5), np.mean(aafa), np.max(aafa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmlab/ksedm1/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "word_embeddings = {i:j for i,j in zip(model.wv.index2word, list(model.wv.vectors))}\n",
    "word_index = {j:i for i,j in enumerate(word_embeddings.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictt3 = {}\n",
    "word_thres = int(np.percentile(aafa, q=99.5))\n",
    "for i,j in dictt2.items():\n",
    "    if len(j) < word_thres:\n",
    "        k = j + ['']*(word_thres-len(j))\n",
    "    else:\n",
    "        k = j[:word_thres]\n",
    "    dictt3[i] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['label'] = pd.factorize(df_train['label'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tid1</th>\n",
       "      <th>tid2</th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>title1_en</th>\n",
       "      <th>title2_en</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017养老保险又新增两项，农村老人人人可申领，你领到了吗</td>\n",
       "      <td>警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京</td>\n",
       "      <td>There are two new old-age insurance benefits f...</td>\n",
       "      <td>Police disprove \"bird's nest congress each per...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>Shenzhen's GDP outstrips Hong Kong? Shenzhen S...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>GDP首超香港？深圳澄清：还差一点点……</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>The GDP overtopped Hong Kong? Shenzhen clarifi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>去年深圳GDP首超香港？深圳统计局辟谣：还差611亿</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>Shenzhen's GDP topped Hong Kong last year? She...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>\"用大蒜鉴别地沟油的方法,怎么鉴别地沟油</td>\n",
       "      <td>吃了30年食用油才知道，一片大蒜轻松鉴别地沟油</td>\n",
       "      <td>\"How to discriminate oil from gutter oil by me...</td>\n",
       "      <td>It took 30 years of cooking oil to know that o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  tid1  tid2                          title1_zh  \\\n",
       "0   0     0     1      2017养老保险又新增两项，农村老人人人可申领，你领到了吗   \n",
       "1   3     2     3  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   \n",
       "2   1     2     4  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   \n",
       "3   2     2     5  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   \n",
       "4   9     6     7               \"用大蒜鉴别地沟油的方法,怎么鉴别地沟油   \n",
       "\n",
       "                    title2_zh  \\\n",
       "0    警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京   \n",
       "1   深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小   \n",
       "2        GDP首超香港？深圳澄清：还差一点点……   \n",
       "3  去年深圳GDP首超香港？深圳统计局辟谣：还差611亿   \n",
       "4     吃了30年食用油才知道，一片大蒜轻松鉴别地沟油   \n",
       "\n",
       "                                           title1_en  \\\n",
       "0  There are two new old-age insurance benefits f...   \n",
       "1  \"If you do not come to Shenzhen, sooner or lat...   \n",
       "2  \"If you do not come to Shenzhen, sooner or lat...   \n",
       "3  \"If you do not come to Shenzhen, sooner or lat...   \n",
       "4  \"How to discriminate oil from gutter oil by me...   \n",
       "\n",
       "                                           title2_en  label  \n",
       "0  Police disprove \"bird's nest congress each per...      0  \n",
       "1  Shenzhen's GDP outstrips Hong Kong? Shenzhen S...      0  \n",
       "2  The GDP overtopped Hong Kong? Shenzhen clarifi...      0  \n",
       "3  Shenzhen's GDP topped Hong Kong last year? She...      0  \n",
       "4  It took 30 years of cooking oil to know that o...      1  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 128\n",
    "        self._load_word_embedding()\n",
    "        self._train_val_split()\n",
    "    \n",
    "    \n",
    "    def _load_word_embedding(self):\n",
    "        self.word_embedding = word_embeddings\n",
    "\n",
    "        \n",
    "    def _simple_attention(self, target, reference):\n",
    "        attention = keras.layers.Dense(1, activation=keras.activations.tanh)(reference)\n",
    "        attention = keras.layers.Reshape((-1,))(attention)\n",
    "        attention = keras.layers.Activation(keras.activations.softmax)(attention)\n",
    "        return keras.layers.Dot((1,1))([target, attention])\n",
    "    \n",
    "        \n",
    "    def _train_val_split(self):\n",
    "        idxs = list(df_train.index)\n",
    "        np.random.shuffle(idxs)\n",
    "        val_num = len(idxs) // 5 \n",
    "        self.train_idxs = idxs[:-val_num]\n",
    "        self.val_idxs = idxs[-val_num:]\n",
    "        self.training_data_size = len(self.train_idxs)\n",
    "        self.validation_data_size = len(self.val_idxs)\n",
    "        \n",
    "        \n",
    "    def _build_model(self):\n",
    "        max_num_word = np.percentile(aafa, q=99.5)\n",
    "        word_embedding_layer = keras.layers.Embedding(\n",
    "            input_dim = len(self.word_embedding),\n",
    "            output_dim = len(list(self.word_embedding.values())[0]),\n",
    "            weights = [np.array(list(self.word_embedding.values()))],\n",
    "            input_length = max_num_word,\n",
    "            trainable = False\n",
    "        )\n",
    "        \n",
    "        dropout = keras.layers.Dropout(0.2)\n",
    "        \n",
    "        word_input = keras.Input((max_num_word,))\n",
    "        word_emb = word_embedding_layer(word_input)\n",
    "        word_cnn = keras.layers.Conv1D(filters=200, kernel_size=5, padding='same', activation='relu', strides=1)(word_emb)\n",
    "        word_att = self._simple_attention(word_cnn, word_cnn)\n",
    "        \n",
    "\n",
    "        word_input2 = keras.Input((max_num_word,))\n",
    "        word_emb2 = word_embedding_layer(word_input2)\n",
    "        word_cnn2 = keras.layers.Conv1D(filters=200, kernel_size=5, padding='same', activation='relu', strides=1)(word_emb2)\n",
    "        word_att2 = self._simple_attention(word_cnn2, word_cnn2)\n",
    "        \n",
    "        word_output = keras.layers.Concatenate()([word_att, word_att2])\n",
    "        \n",
    "        \n",
    "        logits = keras.layers.Dense(3, activation=keras.activations.softmax)(word_output)\n",
    "        model = keras.Model([word_input, word_input2], logits)\n",
    "        model.compile(\n",
    "            optimizer = keras.optimizers.Adam(0.001),\n",
    "            loss = keras.losses.categorical_crossentropy,\n",
    "            metrics = [keras.metrics.categorical_accuracy]\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    \n",
    "    def _training_data_generator(self):\n",
    "        def __gen__():\n",
    "            while True:\n",
    "                for idx in self.train_idxs:\n",
    "                    pair = df_train.iloc[idx]\n",
    "                    text1_ind = [word_index.get(x, 893405) for x in dictt3[pair['tid1']]]\n",
    "                    text2_ind = [word_index.get(x, 893405) for x in dictt3[pair['tid2']]]\n",
    "                    yield text1_ind, text2_ind, pair['label']  \n",
    "                    \n",
    "        gen = __gen__()\n",
    "        \n",
    "        while True:\n",
    "            batch = [np.stack(x) for x in zip(*(next(gen) for _ in range(self.batch_size)))]\n",
    "            yield [batch[0], batch[1]], keras.utils.to_categorical(batch[-1], 3)\n",
    "            \n",
    "    \n",
    "    \n",
    "    def _validation_data_generator(self):\n",
    "        def __gen__():\n",
    "            while True:\n",
    "                for idx in self.val_idxs:\n",
    "                    pair = df_train.iloc[idx]\n",
    "                    text1_ind = [word_index.get(x, 893405) for x in dictt3[pair['tid1']]]\n",
    "                    text2_ind = [word_index.get(x, 893405) for x in dictt3[pair['tid2']]]\n",
    "                    yield text1_ind, text2_ind, pair['label']  \n",
    "                    \n",
    "        gen = __gen__()\n",
    "        \n",
    "        while True:\n",
    "            batch = [np.stack(x) for x in zip(*(next(gen) for _ in range(self.batch_size)))]\n",
    "            yield [batch[0], batch[1]], keras.utils.to_categorical(batch[-1], 3)\n",
    "                \n",
    "    \n",
    "    \n",
    "    def _test_data_generator(self):\n",
    "        def __gen__():\n",
    "            while True:\n",
    "                idxs = list(df_test.index)\n",
    "                for idx in idxs:\n",
    "                    pair = df_test.iloc[idx]\n",
    "                    text1_ind = [word_index.get(x, 893405) for x in dictt3[pair['tid1']]]\n",
    "                    text2_ind = [word_index.get(x, 893405) for x in dictt3[pair['tid2']]]\n",
    "                    yield text1_ind, text2_ind, pair['label']  \n",
    "                    \n",
    "        gen = __gen__()\n",
    "        \n",
    "        while True:\n",
    "            batch = [np.stack(x) for x in zip(*(next(gen) for _ in range(len(df_test))))]\n",
    "            yield [batch[0], batch[1]]  \n",
    "            \n",
    "    \n",
    "    def _test_batch_data(self):\n",
    "        t1 = df_test['tid1'].apply(lambda x: [word_index.get(y, 893405) for y in dictt3[x]]).apply(np.array)\n",
    "        t2 = df_test['tid2'].apply(lambda x: [word_index.get(y, 893405) for y in dictt3[x]]).apply(np.array)\n",
    "        return [np.array(list(t1)), np.array(list(t2))]\n",
    "   \n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        batch_size = 20\n",
    "        self.training_data = self._training_data_generator()\n",
    "        self.validation_data = self._validation_data_generator()\n",
    "        #         self.test_data = self._test_data_generator()\n",
    "        self.test_data = self._test_batch_data()\n",
    "        \n",
    "        self.model = self._build_model()\n",
    "        \n",
    "        self.history = self.model.fit_generator(\n",
    "            generator = self.training_data,\n",
    "            validation_data = self.validation_data,\n",
    "            steps_per_epoch = self.training_data_size // self.batch_size,\n",
    "            validation_steps = self.validation_data_size // self.batch_size,\n",
    "            class_weight = {0:1/16, 1:1/15, 2:1/5},\n",
    "            epochs = 3\n",
    "        )\n",
    "      \n",
    "        \n",
    "        self.result = self.model.predict_on_batch(self.test_data)\n",
    "           \n",
    "#         self.result = self.model.predict_generator(\n",
    "#             generator = self.test_data,\n",
    "#             steps = 1\n",
    "#         )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "2003/2003 [==============================] - 109s 55ms/step - loss: 0.0361 - categorical_accuracy: 0.7531 - val_loss: 0.4294 - val_categorical_accuracy: 0.7951\n",
      "Epoch 2/3\n",
      "2003/2003 [==============================] - 107s 54ms/step - loss: 0.0282 - categorical_accuracy: 0.8196 - val_loss: 0.4062 - val_categorical_accuracy: 0.8121\n",
      "Epoch 3/3\n",
      "2003/2003 [==============================] - 108s 54ms/step - loss: 0.0239 - categorical_accuracy: 0.8519 - val_loss: 0.3941 - val_categorical_accuracy: 0.8224\n"
     ]
    }
   ],
   "source": [
    "r = Prediction()\n",
    "r.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## validation accuacy가 가장 높은 epoch의 모델로 test data를 평가하는 걸로 코드 전환 - 모델 save or stopping condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unrelated    56037\n",
      "agreed       22021\n",
      "disagreed     2068\n",
      "Name: pred_label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_test['pred_label'] = r.result.argmax(axis=1)\n",
    "idx_label = {0:'unrelated', 1:'agreed', 2:'disagreed'}\n",
    "df_test['pred_label'] = df_test['pred_label'].apply(lambda x: idx_label[x])\n",
    "print(df_test['pred_label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_path = '../data/sample_submission_sd_weighted.csv'\n",
    "df_test[['id', 'pred_label']].rename(columns={'id':'Id','pred_label':'Category'}).to_csv(submission_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    219313\n",
       "1     92973\n",
       "2      8266\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 167563, 17, 189305)"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(df_train.tid1), max(df_train.tid1), min(df_test.tid1), max(df_test.tid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 167558, 3, 189303)"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(df_train.tid2), max(df_train.tid2), min(df_test.tid2), max(df_test.tid2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
